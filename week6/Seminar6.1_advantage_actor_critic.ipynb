{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage actor-critic in AgentNet (5 pts)\n",
    "\n",
    "Once we're done with REINFORCE, it's time to proceed with something more sophisticated.\n",
    "The next one in line is advantage actor-critic, in which agent learns both policy and value function, using the latter to speed up learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:02:34,564] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.40807951  0.        ]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "obs = env.reset()\n",
    "state_size = len(obs)\n",
    "n_actions = env.action_space.n\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,NonlinearityLayer,batch_norm,dropout\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,state_size))\n",
    "\n",
    "nn = DenseLayer(observation_layer,256)#<your architecture>\n",
    "nn = DenseLayer(nn,256)#<your architecture>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "\n",
    "policy_layer = DenseLayer(nn,n_actions,nonlinearity=lasagne.nonlinearities.softmax)#<estimate probabilities of actions given prev layer. Mind the nonlinearity!>\n",
    "\n",
    "\n",
    "V_layer = DenseLayer(nn,1,nonlinearity=None)#<estimate state values (1 unit layer). Mind nonlinearity too.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "action_layer = ProbabilisticResolver(policy_layer,\n",
    "                                     name=\"e-greedy action picker\",\n",
    "                                     assume_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(policy_layer,V_layer),\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((action_layer,V_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:02:35,582] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,585] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,587] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,589] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,591] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,593] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,595] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,598] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,600] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:35,602] Making new env: MountainCar-v0\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "#create a small pool with 10 parallel agents\n",
    "pool = EnvPool(agent,\"MountainCar-v0\", n_games=10,max_size=1000) \n",
    "\n",
    "#we assume that pool size 1000 is small enough to learn \"almost on policy\" :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 1 2 1 0 0]\n",
      " [0 2 0 2 0 2 1]\n",
      " [0 1 2 1 0 1 0]]\n",
      "[[-1. -1. -1. -1. -1. -1.  0.]\n",
      " [-1. -1. -1. -1. -1. -1.  0.]\n",
      " [-1. -1. -1. -1. -1. -1.  0.]]\n",
      "CPU times: user 7.16 ms, sys: 0 ns, total: 7.16 ms\n",
      "Wall time: 6.97 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_log[:3])\n",
    "print(reward_log[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 10\n",
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic loss\n",
    "\n",
    "Here we define obective function for actor-critic (one-step) RL.\n",
    "\n",
    "* We regularize policy with expected inverse action probabilities (discouraging very small probas) to make objective numerically stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay\n",
    "\n",
    "_,_,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning import a2c                                                   \n",
    "\n",
    "\n",
    "elwise_mse_loss = a2c.get_elementwise_objective(policy_seq,\n",
    "                                                V_seq[:,:,0],\n",
    "                                                replay.actions[0],\n",
    "                                                replay.rewards,\n",
    "                                                replay.is_alive,\n",
    "                                                gamma_or_gammas=0.99,\n",
    "                                                n_steps=1)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "reg_entropy = (1./(policy_seq)).sum(-1).mean()#<regularize agent with 1/pi. Higher entropy = smaller loss. >\n",
    "\n",
    "loss += 0.01*reg_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.rmsprop(loss,weights,learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:02:44,437] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:02:44,440] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:02:44,441] Clearing 3 monitor files from previous run (because force=True was provided)\n",
      "[2017-03-15 03:02:44,443] Starting new video recorder writing to /root/anet/Practical_RL/week6/records/openaigym.video.0.3558.video000000.mp4\n",
      "[2017-03-15 03:03:11,264] Tried to pass invalid video frame, marking as broken: Your frame has shape (1, 1, 3), but the VideoRecorder is configured for shape (400, 600, 3).\n",
      "[2017-03-15 03:03:11,362] Cleaning up paths for broken video recorder: path=/root/anet/Practical_RL/week6/records/openaigym.video.0.3558.video000000.mp4 metadata_path=/root/anet/Practical_RL/week6/records/openaigym.video.0.3558.video000000.meta.json\n",
      "[2017-03-15 03:03:11,384] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 200 timesteps with reward=-200.0\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n",
    "\n",
    "#video is in the ./records folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 101/10000 [00:06<18:53,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=100\treward/step=-1.00000\tloss ma=-0.12804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 201/10000 [00:19<19:20,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=200\treward/step=-1.00000\tloss ma=0.50704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 301/10000 [00:31<19:14,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=300\treward/step=-0.99989\tloss ma=0.92701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 401/10000 [00:44<19:55,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=400\treward/step=-0.99989\tloss ma=1.17698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 499/10000 [00:56<19:06,  8.29it/s][2017-03-15 03:04:08,617] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:04:08,620] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:04:08,622] Clearing 3 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=500\treward/step=-1.00000\tloss ma=0.66480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:04:09,115] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      "  5%|▌         | 501/10000 [00:57<41:01,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 601/10000 [01:10<20:56,  7.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=600\treward/step=-1.00000\tloss ma=0.31583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 701/10000 [01:23<20:24,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=700\treward/step=-1.00000\tloss ma=0.17722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 801/10000 [01:36<20:48,  7.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=800\treward/step=-1.00000\tloss ma=0.12628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 901/10000 [01:48<19:57,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=900\treward/step=-1.00000\tloss ma=0.10716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 999/10000 [02:00<21:48,  6.88it/s][2017-03-15 03:05:12,619] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:05:12,623] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:05:12,624] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1000\treward/step=-1.00000\tloss ma=0.09998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:05:13,124] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 10%|█         | 1002/10000 [02:01<30:21,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1101/10000 [02:14<17:30,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1100\treward/step=-1.00000\tloss ma=0.09730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1201/10000 [02:26<18:16,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1200\treward/step=-1.00000\tloss ma=0.09622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1301/10000 [02:39<18:06,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1300\treward/step=-0.99994\tloss ma=0.18702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1401/10000 [02:51<17:13,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1400\treward/step=-1.00000\tloss ma=0.28617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1499/10000 [03:03<17:11,  8.24it/s][2017-03-15 03:06:15,442] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:06:15,446] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:06:15,448] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1500\treward/step=-0.99994\tloss ma=0.22260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:06:15,972] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 15%|█▌        | 1501/10000 [03:04<35:01,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1601/10000 [03:16<16:18,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1600\treward/step=-0.99983\tloss ma=0.38983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1701/10000 [03:28<16:40,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1700\treward/step=-0.99994\tloss ma=0.32245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1801/10000 [03:41<16:37,  8.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1800\treward/step=-0.99989\tloss ma=0.32253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1902/10000 [03:53<16:19,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1900\treward/step=-0.99978\tloss ma=0.35401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1999/10000 [04:06<15:58,  8.35it/s][2017-03-15 03:07:17,970] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:07:17,973] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:07:17,975] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2000\treward/step=-0.99956\tloss ma=0.43938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:07:18,458] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 20%|██        | 2001/10000 [04:07<32:16,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -200.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 2101/10000 [04:19<16:07,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2100\treward/step=-0.99911\tloss ma=1.23256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2201/10000 [04:32<16:38,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2200\treward/step=-0.99878\tloss ma=1.24214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2301/10000 [04:45<15:50,  8.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2300\treward/step=-0.99805\tloss ma=1.15351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 2401/10000 [04:57<14:40,  8.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2400\treward/step=-0.99749\tloss ma=1.05558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2499/10000 [05:10<15:24,  8.12it/s][2017-03-15 03:08:21,756] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:08:21,759] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:08:21,760] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2500\treward/step=-0.99738\tloss ma=0.89084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:08:22,280] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 25%|██▌       | 2501/10000 [05:10<30:13,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -180.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 2601/10000 [05:23<15:54,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2600\treward/step=-0.99727\tloss ma=0.85026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 2701/10000 [05:35<14:59,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2700\treward/step=-0.99727\tloss ma=0.83069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 2801/10000 [05:48<16:18,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2800\treward/step=-0.99733\tloss ma=0.78665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 2901/10000 [06:00<16:39,  7.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=2900\treward/step=-0.99727\tloss ma=0.77308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2999/10000 [06:13<14:52,  7.85it/s][2017-03-15 03:09:25,329] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:09:25,332] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:09:25,333] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3000\treward/step=-0.99705\tloss ma=0.80363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:09:25,802] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 30%|███       | 3001/10000 [06:14<27:44,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -178.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 3101/10000 [06:27<14:15,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3100\treward/step=-0.99710\tloss ma=0.76121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 3201/10000 [06:39<15:28,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3200\treward/step=-0.99727\tloss ma=0.75326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3301/10000 [06:52<13:42,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3300\treward/step=-0.99727\tloss ma=0.74640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 3401/10000 [07:05<13:22,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3400\treward/step=-0.99705\tloss ma=0.73848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3499/10000 [07:17<12:55,  8.38it/s][2017-03-15 03:10:29,243] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:10:29,246] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:10:29,247] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3500\treward/step=-0.99716\tloss ma=0.69179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:10:29,637] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 35%|███▌      | 3501/10000 [07:18<21:49,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -161.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 3601/10000 [07:31<14:17,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3600\treward/step=-0.99710\tloss ma=0.70496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 3701/10000 [07:43<13:02,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3700\treward/step=-0.99710\tloss ma=0.62804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 3801/10000 [07:55<12:53,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3800\treward/step=-0.99716\tloss ma=0.65986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 3901/10000 [08:08<12:39,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=3900\treward/step=-0.99710\tloss ma=0.60895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 3999/10000 [08:21<12:14,  8.17it/s][2017-03-15 03:11:32,947] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:11:32,950] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:11:32,951] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4000\treward/step=-0.99716\tloss ma=0.58934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:11:33,356] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 40%|████      | 4002/10000 [08:22<18:18,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -157.500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 4101/10000 [08:34<12:30,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4100\treward/step=-0.99694\tloss ma=0.59245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4201/10000 [08:46<13:02,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4200\treward/step=-0.99716\tloss ma=0.57948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 4301/10000 [08:59<12:21,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4300\treward/step=-0.99699\tloss ma=0.58098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4402/10000 [09:12<11:01,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4400\treward/step=-0.99716\tloss ma=0.54602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4499/10000 [09:24<10:43,  8.55it/s][2017-03-15 03:12:35,964] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:12:35,966] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:12:35,967] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4500\treward/step=-0.99733\tloss ma=0.55025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:12:36,376] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 45%|████▌     | 4501/10000 [09:24<18:38,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -165.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 4601/10000 [09:37<12:21,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4600\treward/step=-0.99710\tloss ma=0.54896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 4701/10000 [09:49<10:29,  8.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4700\treward/step=-0.99705\tloss ma=0.55388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4801/10000 [10:01<10:36,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4800\treward/step=-0.99716\tloss ma=0.51218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 4901/10000 [10:14<11:03,  7.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=4900\treward/step=-0.99716\tloss ma=0.49164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 4999/10000 [10:26<10:26,  7.98it/s][2017-03-15 03:13:38,324] Making new env: MountainCar-v0\n",
      "[2017-03-15 03:13:38,328] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-15 03:13:38,329] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5000\treward/step=-0.99721\tloss ma=0.48065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-15 03:13:38,777] Finished writing results. You can upload them to the scoreboard via gym.upload('/root/anet/Practical_RL/week6/records')\n",
      " 50%|█████     | 5002/10000 [10:27<15:57,  5.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -166.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 5101/10000 [10:39<10:09,  8.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5100\treward/step=-0.99705\tloss ma=0.52175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5201/10000 [10:52<09:28,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5200\treward/step=-0.99716\tloss ma=0.49839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 5301/10000 [11:05<09:11,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5300\treward/step=-0.99721\tloss ma=0.45238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5401/10000 [11:17<09:01,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=5400\treward/step=-0.99716\tloss ma=0.45623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 5415/10000 [11:19<08:55,  8.57it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "loss = 0\n",
    "for i in tqdm(range(10000)):    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    loss = loss*0.99 + train_step()*0.01\n",
    "        \n",
    "    \n",
    "    \n",
    "    if epoch_counter%100==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = np.average(pool.experience_replay.rewards.get_value()[:,:-1],\n",
    "                                      weights=1+pool.experience_replay.is_alive.get_value()[:,:-1])\n",
    "        print(\"iter=%i\\treward/step=%.5f\\tloss ma=%.5f\"%(epoch_counter,\n",
    "                                                        pool_mean_reward,\n",
    "                                                        loss))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,\n",
    "                                               verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iters,map(np.mean,session_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the $V(s)$ and  $\\pi(a|s)$\n",
    "\n",
    "Since the observation space is just 2-dimensional, we can plot it on a 2d scatter-plot to gain insight of what agent learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_,_,_,_,(pool_policy,pool_V) = agent.get_sessions(\n",
    "    pool.experience_replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,)\n",
    "\n",
    "plt.scatter(\n",
    "    *pool.experience_replay.observations[0].get_value().reshape([-1,2]).T,\n",
    "    c = pool_V.ravel().eval(),\n",
    "    alpha = 0.1)\n",
    "plt.title(\"predicted state values\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "obs_x,obs_y = pool.experience_replay.observations[0].get_value().reshape([-1,2]).T\n",
    "optimal_actid = pool_policy.argmax(-1).ravel().eval()\n",
    "action_names=[\"left\",\"stop\",\"right\"]\n",
    "for i in range(3):\n",
    "    sel = optimal_actid==i\n",
    "    plt.scatter(obs_x[sel],obs_y[sel],\n",
    "                c=['red','blue','green'][i],\n",
    "                alpha = 0.1,label=action_names[i])\n",
    "    \n",
    "plt.title(\"most likely action id\")\n",
    "plt.xlabel(\"position\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations in the algorithm (2 pts)\n",
    "\n",
    "Try different `n_steps` param to see if it improves learning performance.\n",
    "\n",
    "Your objective is to compare learning curves for 1, 3, 10 and 25-step updates (or any grid you think is appropriate).\n",
    "\n",
    "For 25-step updates, please also increase SEQ_LENGTH to 25.\n",
    "\n",
    "_(Bonus)_ Also evaluate how performance changes with different entropy regularizer coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<a lot of your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus section (5+ pts)\n",
    "\n",
    "Beat the [`LunarLanderContinuous-v2`](https://gym.openai.com/envs/LunarLanderContinuous-v2) with continuous version of advantage actor-critic.\n",
    "\n",
    "You will require a multidimensional gaussian (or similar) policy from your agent.\n",
    "\n",
    "You can implement that by feeding a2c.get_elementwise_objective probabilities of agent's chosen actions (it will be 2-dimensional) instead of all actions.\n",
    "\n",
    "Contact us if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
