{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage actor-critic in AgentNet (5 pts)\n",
    "\n",
    "Once we're done with REINFORCE, it's time to proceed with something more sophisticated.\n",
    "The next one in line is advantage actor-critic, in which agent learns both policy and value function, using the latter to speed up learning.\n",
    "\n",
    "We'll start as usual by running LunarLander env. If you didn't manage to get it installed by now, ~~you deserve whatever happens to you~~ you can try `MountainCar-v0` or `Acrobot-v0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1\n",
    "        \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:05,109] Making new env: LunarLander-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00328159  0.94092541  0.33237436  0.02080304 -0.00379576 -0.07528779\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "obs = env.reset()\n",
    "state_size = len(obs)\n",
    "n_actions = env.action_space.n\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic agent setup\n",
    "Here we define a simple agent that maps game images into Qvalues using shallow neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import InputLayer,DenseLayer,NonlinearityLayer,batch_norm,dropout\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,state_size))\n",
    "\n",
    "nn = DenseLayer(observation_layer,100)#<your architecture>\n",
    "nn = DenseLayer(nn,200)#<your architecture>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#a layer that predicts Qvalues\n",
    "\n",
    "policy_layer = DenseLayer(nn,n_actions,nonlinearity=lasagne.nonlinearities.softmax)#<estimate probabilities of actions given prev layer. Mind the nonlinearity!>\n",
    "\n",
    "\n",
    "V_layer = DenseLayer(nn,1,nonlinearity=None)#<estimate state values (1 unit layer). Mind nonlinearity too.>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To pick actions, we use an epsilon-greedy resolver (epsilon is a property)\n",
    "from agentnet.resolver import ProbabilisticResolver\n",
    "action_layer = ProbabilisticResolver(policy_layer,\n",
    "                                     name=\"e-greedy action picker\",\n",
    "                                     assume_normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally, agent\n",
    "We declare that this network is and MDP agent with such and such inputs, states and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "#all together\n",
    "agent = Agent(observation_layers=observation_layer,\n",
    "              policy_estimators=(policy_layer,V_layer),\n",
    "              action_layers=action_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[W, b, W, b, W, b, W, b]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params((action_layer,V_layer),trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,697] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,709] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,720] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,730] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,742] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,752] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,762] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,773] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,783] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:06,793] Making new env: LunarLander-v2\n"
     ]
    }
   ],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "\n",
    "#create a small pool with 10 parallel agents\n",
    "pool = EnvPool(agent,\"LunarLander-v2\", n_games=10,max_size=1000) \n",
    "\n",
    "#we assume that pool size 1000 is small enough to learn \"almost on policy\" :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 1 3 2 0 0]\n",
      " [0 2 0 3 1 3 3]\n",
      " [0 1 3 1 0 1 1]]\n",
      "[[-0.02555107  0.73051758  1.67219392 -0.05185902 -1.55174084  0.68232006\n",
      "   0.        ]\n",
      " [ 0.89301952 -5.05226646  0.95141698  1.94756751  0.51880214  1.94206123\n",
      "   0.        ]\n",
      " [-0.54613835  0.41520672 -1.60931248  0.50348755 -0.65353357  0.35359821\n",
      "   0.        ]]\n",
      "CPU times: user 24 ms, sys: 32 ms, total: 56 ms\n",
      "Wall time: 56.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "_,action_log,reward_log,_,_,_  = pool.interact(7)\n",
    "\n",
    "\n",
    "print(action_log[:3])\n",
    "print(reward_log[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEQ_LENGTH = 10\n",
    "#load first sessions (this function calls interact and remembers sessions)\n",
    "pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-critic loss\n",
    "\n",
    "Here we define obective function for actor-critic (one-step) RL.\n",
    "\n",
    "* We regularize policy with expected inverse action probabilities (discouraging very small probas) to make objective numerically stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay\n",
    "\n",
    "_,_,_,_,(policy_seq,V_seq) = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.learning import a2c                                                   \n",
    "\n",
    "\n",
    "elwise_mse_loss = a2c.get_elementwise_objective(policy_seq,\n",
    "                                                V_seq[:,:,0],\n",
    "                                                replay.actions[0],\n",
    "                                                replay.rewards*0.01,\n",
    "                                                replay.is_alive,\n",
    "                                                gamma_or_gammas=0.99,\n",
    "                                                n_steps=1)\n",
    "\n",
    "#compute mean over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano import tensor as T\n",
    "reg_entropy = (policy_seq*T.log(policy_seq)).sum(-1).mean()#<regularize agent with __negative__ entropy. Higher entropy = smaller loss. >\n",
    "\n",
    "loss += reg_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file blas.py, line 433\n",
      "[2017-03-15 01:19:24,257] We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    }
   ],
   "source": [
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:19:27,106] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 81\n",
      "[2017-03-15 01:19:27,118] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 31\n",
      "[2017-03-15 01:19:27,120] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file video_recorder.py, line 93\n",
      "[2017-03-15 01:19:27,125] Starting new video recorder writing to /home/jheuristic/Downloads/Practical_RL/week6/records/openaigym.video.0.14833.video000000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 94 timesteps with reward=-250.488916479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 221\n",
      "[2017-03-15 01:19:32,627] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/Practical_RL/week6/records')\n"
     ]
    }
   ],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)\n",
    "\n",
    "#video is in the ./records folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#starting epoch\n",
    "epoch_counter = 1\n",
    "\n",
    "#full game rewards\n",
    "rewards = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10000 [00:36<1:23:52,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=100\treward/step=-2.22627\tpool_size=1000\tloss ma=-0.88251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 200/10000 [01:24<1:06:05,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=200\treward/step=-2.35537\tpool_size=1000\tloss ma=-1.19674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 300/10000 [02:09<1:17:07,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=300\treward/step=-2.43342\tpool_size=1000\tloss ma=-1.31218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 400/10000 [03:03<1:51:59,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=400\treward/step=-2.56407\tpool_size=1000\tloss ma=-1.35477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 499/10000 [03:53<1:16:19,  2.07it/s]Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:23:26,824] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 81\n",
      "[2017-03-15 01:23:26,858] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 31\n",
      "[2017-03-15 01:23:26,862] Clearing 4 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=500\treward/step=-2.32853\tpool_size=1000\tloss ma=-1.36896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 221\n",
      "[2017-03-15 01:23:27,893] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/Practical_RL/week6/records')\n",
      "  5%|▌         | 500/10000 [03:55<2:05:57,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -173.312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 600/10000 [04:41<1:29:45,  1.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=600\treward/step=-2.11058\tpool_size=1000\tloss ma=-1.37464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 700/10000 [05:40<1:57:35,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=700\treward/step=-2.36339\tpool_size=1000\tloss ma=-1.37662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 800/10000 [06:41<1:22:14,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=800\treward/step=-1.94985\tpool_size=1000\tloss ma=-1.37838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 900/10000 [07:34<1:30:23,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=900\treward/step=-1.96288\tpool_size=1000\tloss ma=-1.38105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 999/10000 [08:52<1:51:46,  1.34it/s]Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:28:25,739] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 81\n",
      "[2017-03-15 01:28:25,765] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 31\n",
      "[2017-03-15 01:28:25,771] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1000\treward/step=-2.13598\tpool_size=1000\tloss ma=-1.38227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 221\n",
      "[2017-03-15 01:28:26,859] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/Practical_RL/week6/records')\n",
      " 10%|█         | 1000/10000 [08:54<2:42:23,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -219.689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1100/10000 [10:22<2:16:51,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1100\treward/step=-1.96493\tpool_size=1000\tloss ma=-1.38154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 1200/10000 [12:04<2:14:45,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1200\treward/step=-1.83011\tpool_size=1000\tloss ma=-1.38223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 1300/10000 [14:00<2:54:59,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1300\treward/step=-1.61950\tpool_size=1000\tloss ma=-1.38228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1400/10000 [16:15<4:16:25,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1400\treward/step=-1.65189\tpool_size=1000\tloss ma=-1.38259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1499/10000 [18:45<3:28:51,  1.47s/it]Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 103\n",
      "[2017-03-15 01:38:19,500] Making new env: LunarLander-v2\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file registration.py, line 81\n",
      "[2017-03-15 01:38:19,508] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 31\n",
      "[2017-03-15 01:38:19,511] Clearing 2 monitor files from previous run (because force=True was provided)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1500\treward/step=-1.83839\tpool_size=1000\tloss ma=-1.38299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jheuristic/anaconda2/lib/python2.7/logging/__init__.py\", line 882, in emit\n",
      "    stream.write(fs % msg)\n",
      "IOError: [Errno 5] Input/output error\n",
      "Logged from file monitor_manager.py, line 221\n",
      "[2017-03-15 01:38:20,461] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Downloads/Practical_RL/week6/records')\n",
      " 15%|█▌        | 1500/10000 [18:47<4:07:45,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current score(mean over 10) = -228.532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1600/10000 [21:42<3:53:34,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1600\treward/step=-1.67259\tpool_size=1000\tloss ma=-1.38294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1700/10000 [26:03<5:09:27,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1700\treward/step=-1.76617\tpool_size=1000\tloss ma=-1.38275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1800/10000 [29:11<3:43:59,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1800\treward/step=-1.82253\tpool_size=1000\tloss ma=-1.38253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1892/10000 [31:45<3:53:20,  1.73s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#the loop may take eons to finish.\n",
    "#consider interrupting early.\n",
    "loss = 0\n",
    "for i in tqdm(range(10000)):    \n",
    "    \n",
    "    #train\n",
    "    pool.update(SEQ_LENGTH,append=True,)\n",
    "    \n",
    "    loss = loss*0.99 + train_step()*0.01\n",
    "        \n",
    "    \n",
    "    \n",
    "    if epoch_counter%100==0:\n",
    "        #average reward per game tick in current experience replay pool\n",
    "        pool_mean_reward = np.average(pool.experience_replay.rewards.get_value()[:,:-1],\n",
    "                                      weights=1+pool.experience_replay.is_alive.get_value()[:,:-1])\n",
    "        print(\"iter=%i\\treward/step=%.5f\\loss ma=%.5f\"%(epoch_counter,\n",
    "                                                        pool_mean_reward,\n",
    "                                                        loss))\n",
    "        \n",
    "\n",
    "    ##record current learning progress and show learning curves\n",
    "    if epoch_counter%500 ==0:\n",
    "        n_games = 10\n",
    "        rewards[epoch_counter] = pool.evaluate( record_video=False,n_games=n_games,\n",
    "                                               verbose=False)\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(n_games,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(iters,map(np.mean,session_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variations in the algorithm (2 pts)\n",
    "\n",
    "Try different `n_steps` param to see if it improves learning performance.\n",
    "\n",
    "Your objective is to compare learning curves for 1, 3, 10 and 25-step updates (or any grid you think is appropriate).\n",
    "\n",
    "For 25-step updates, please also increase SEQ_LENGTH to 25.\n",
    "\n",
    "_(Bonus)_ Also evaluate how performance changes with different entropy regularizer coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<a lot of your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus section (5+ pts)\n",
    "\n",
    "Beat the [`LunarLanderContinuous-v2`](https://gym.openai.com/envs/LunarLanderContinuous-v2) with continuous version of advantage actor-critic.\n",
    "\n",
    "You will require a multidimensional gaussian (or similar) policy from your agent.\n",
    "\n",
    "You can implement that by feeding a2c.get_elementwise_objective probabilities of agent's chosen actions (it will be 2-dimensional) instead of all actions.\n",
    "\n",
    "Contact us if you have any questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
